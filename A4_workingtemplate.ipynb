{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/macsyd/GPU-Computing/blob/main/A4_workingtemplate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the nvcc4jupyter extension for CUDA compilation in Jupyter/Colab\n",
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
        "\n",
        "# Load the extension\n",
        "%load_ext nvcc4jupyter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-s38NPp7Yznh",
        "outputId": "4bf52431-9812-4165-bc4e-96d749216bf7"
      },
      "id": "-s38NPp7Yznh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-6lse80fl\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-6lse80fl\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit 28f872a2f99a1b201bcd0db14fdbc5a496b9bfd7\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: nvcc4jupyter\n",
            "  Building wheel for nvcc4jupyter (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvcc4jupyter: filename=nvcc4jupyter-1.2.1-py3-none-any.whl size=10733 sha256=25d7336482e00933626da974c4579936e1bcd2169b147b54a7d099795d56a4e1\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-e8h8sjhc/wheels/a8/b9/18/23f8ef71ceb0f63297dd1903aedd067e6243a68ea756d6feea\n",
            "Successfully built nvcc4jupyter\n",
            "Installing collected packages: nvcc4jupyter\n",
            "Successfully installed nvcc4jupyter-1.2.1\n",
            "Detected platform \"Colab\". Running its setup...\n",
            "Source files will be saved in \"/tmp/tmp9h3k53d5\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda_group_save -g \"source\" -n \"error_checking.h\"\n",
        "\n",
        "// Define some error checking macros.\n",
        "#define cudaErrCheck(stat) { cudaErrCheck_((stat), __FILE__, __LINE__); }\n",
        "void cudaErrCheck_(cudaError_t stat, const char *file, int line) {\n",
        "   if (stat != cudaSuccess) {\n",
        "      fprintf(stderr, \"CUDA Error: %s %s %d\\n\", cudaGetErrorString(stat), file, line);\n",
        "   }\n",
        "}\n",
        "\n",
        "#define cublasErrCheck(stat) { cublasErrCheck_((stat), __FILE__, __LINE__); }\n",
        "void cublasErrCheck_(cublasStatus_t stat, const char *file, int line) {\n",
        "   if (stat != CUBLAS_STATUS_SUCCESS) {\n",
        "      fprintf(stderr, \"cuBLAS Error: %d %s %d\\n\", stat, file, line);\n",
        "   }\n",
        "}\n",
        "\n",
        "#define curandErrCheck(stat) { curandErrCheck_((stat), __FILE__, __LINE__); }\n",
        "void curandErrCheck_(curandStatus_t stat, const char *file, int line) {\n",
        "   if (stat != CURAND_STATUS_SUCCESS) {\n",
        "      fprintf(stderr, \"cuRand Error: %d %s %d\\n\", stat, file, line);\n",
        "   }\n",
        "}"
      ],
      "metadata": {
        "id": "wCmzq-3BnNB0"
      },
      "id": "wCmzq-3BnNB0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda_group_save -g \"source\" -n \"wmma_fp16_kernel.h\"\n",
        "\n",
        "/* THE FOLLOWING CODE TAKEN FROM THE FOLLOWING RESOURCE BY NVIDIA:\n",
        " * https://github.com/NVIDIA-developer-blog/code-samples/blob/master/posts/tensor-cores/simpleTensorCoreGEMM.cu\n",
        "*/\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <curand.h>\n",
        "#include <cublas_v2.h>\n",
        "\n",
        "#include <mma.h>\n",
        "using namespace nvcuda;\n",
        "\n",
        "// GEMM for half precision values (i.e., fp16)\n",
        "__global__ void gemm_wmma_half(half* c, int M, int N, int K, half* a, half* b) {\n",
        "    // Matrix dimensions for WMMA (16x16x16 tiles)\n",
        "    const int WMMA_M = 16;\n",
        "    const int WMMA_N = 16;\n",
        "    const int WMMA_K = 16;\n",
        "\n",
        "    // Leading dimensions\n",
        "    int lda = M;\n",
        "    int ldb = K;\n",
        "    int ldc = M;\n",
        "\n",
        "    // Tile using a 2D grid\n",
        "    int warpM = (blockIdx.x * blockDim.x + threadIdx.x) / warpSize;\n",
        "    int warpN = (blockIdx.y * blockDim.y + threadIdx.y);\n",
        "\n",
        "    // Declare fragments\n",
        "    wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> a_frag;\n",
        "    wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::col_major> b_frag;\n",
        "    wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, half> acc_frag;\n",
        "    wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, half> c_frag;\n",
        "\n",
        "    // Initialize accumulator to 0\n",
        "    wmma::fill_fragment(acc_frag, 0.0f);\n",
        "\n",
        "    for (int i = 0; i < K; i += WMMA_K) {\n",
        "      int aRow = warpM * WMMA_M;\n",
        "      int aCol = i;\n",
        "\n",
        "      int bRow = i;\n",
        "      int bCol = warpN * WMMA_N;\n",
        "\n",
        "      // Ensure only threads within bounds execute MMAs\n",
        "      if (aRow < M && aCol < K && bRow < K && bCol < N) {\n",
        "         /*\n",
        "         // Load the inputs\n",
        "         wmma::load_matrix_sync(a_frag, a + aRow + aCol * lda, lda);\n",
        "         wmma::load_matrix_sync(b_frag, b + bRow + bCol * ldb, ldb);\n",
        "         */\n",
        "\n",
        "         // Fill inputs with 1s for testing\n",
        "         wmma::fill_fragment(a_frag, __float2half(1.0f));  // Fill matrix A (with 1s for testing)\n",
        "         wmma::fill_fragment(b_frag, __float2half(1.0f));  // Fill matrix B (with 1s for testing)\n",
        "\n",
        "         // Perform the matrix multiplication\n",
        "         wmma::mma_sync(acc_frag, a_frag, b_frag, acc_frag);\n",
        "      }\n",
        "   }\n",
        "\n",
        "    // Load in the current value of c and add this to our result\n",
        "    int cRow = warpM * WMMA_M;\n",
        "    int cCol = warpN * WMMA_N;\n",
        "\n",
        "    if (cRow < M && cCol < N) {\n",
        "       wmma::load_matrix_sync(c_frag, c + cRow + cCol * ldc, ldc, wmma::mem_col_major);\n",
        "\n",
        "#pragma unroll\n",
        "       for(int i=0; i < c_frag.num_elements; i++) {\n",
        "          c_frag.x[i] = acc_frag.x[i] + c_frag.x[i];\n",
        "       }\n",
        "\n",
        "       // Store the output (print in main)\n",
        "       wmma::store_matrix_sync(c + cRow + cCol * ldc, c_frag, ldc, wmma::mem_col_major);\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "btmk9_BooP80"
      },
      "id": "btmk9_BooP80",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda_group_save -g \"source\" -n \"wmma_fp32_kernel.h\"\n",
        "\n",
        "/* THE FOLLOWING CODE TAKEN FROM THE FOLLOWING RESOURCE BY NVIDIA:\n",
        " * https://github.com/NVIDIA-developer-blog/code-samples/blob/master/posts/tensor-cores/simpleTensorCoreGEMM.cu\n",
        "*/\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <curand.h>\n",
        "#include <cublas_v2.h>\n",
        "\n",
        "#include <mma.h>\n",
        "using namespace nvcuda;\n",
        "\n",
        "// GEMM for full precision values (i.e., fp32)\n",
        "__global__ void gemm_wmma_full(float* c, int M, int N, int K, float* a, float* b) {\n",
        "    // Matrix dimensions for WMMA (16x16x16 tiles)\n",
        "    const int WMMA_M = 16;\n",
        "    const int WMMA_N = 16;\n",
        "    const int WMMA_K = 16;\n",
        "\n",
        "    // Leading dimensions\n",
        "    int lda = M;\n",
        "    int ldb = K;\n",
        "    int ldc = M;\n",
        "\n",
        "    // Tile using a 2D grid\n",
        "    int warpM = (blockIdx.x * blockDim.x + threadIdx.x) / warpSize;\n",
        "    int warpN = (blockIdx.y * blockDim.y + threadIdx.y);\n",
        "\n",
        "    // Declare fragments\n",
        "    wmma::fragment<wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, half, wmma::row_major> a_frag;\n",
        "    wmma::fragment<wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, half, wmma::col_major> b_frag;\n",
        "    wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, float> acc_frag;\n",
        "    wmma::fragment<wmma::accumulator, WMMA_M, WMMA_N, WMMA_K, float> c_frag;\n",
        "\n",
        "    // Initialize accumulator to 0\n",
        "    wmma::fill_fragment(acc_frag, 0.0f);\n",
        "\n",
        "    for (int i = 0; i < K; i += WMMA_K) {\n",
        "      int aRow = warpM * WMMA_M;\n",
        "      int aCol = i;\n",
        "\n",
        "      int bRow = i;\n",
        "      int bCol = warpN * WMMA_N;\n",
        "\n",
        "      // Ensure only threads within bounds execute MMAs\n",
        "      if (aRow < M && aCol < K && bRow < K && bCol < N) {\n",
        "         /*\n",
        "         // Load the inputs\n",
        "         wmma::load_matrix_sync(a_frag, a + aRow + aCol * lda, lda);\n",
        "         wmma::load_matrix_sync(b_frag, b + bRow + bCol * ldb, ldb);\n",
        "         */\n",
        "\n",
        "         // Fill inputs with 1s for testing\n",
        "         wmma::fill_fragment(a_frag, 1.0f);  // Fill matrix A (with 1s for testing)\n",
        "         wmma::fill_fragment(b_frag, 1.0f);  // Fill matrix B (with 1s for testing)\n",
        "\n",
        "         // Perform the matrix multiplication\n",
        "         wmma::mma_sync(acc_frag, a_frag, b_frag, acc_frag);\n",
        "      }\n",
        "   }\n",
        "\n",
        "    // Load in the current value of c and add this to our result\n",
        "    int cRow = warpM * WMMA_M;\n",
        "    int cCol = warpN * WMMA_N;\n",
        "\n",
        "    if (cRow < M && cCol < N) {\n",
        "       wmma::load_matrix_sync(c_frag, c + cRow + cCol * ldc, ldc, wmma::mem_col_major);\n",
        "\n",
        "#pragma unroll\n",
        "       for(int i=0; i < c_frag.num_elements; i++) {\n",
        "          c_frag.x[i] = acc_frag.x[i] + c_frag.x[i];\n",
        "       }\n",
        "\n",
        "       // Store the output (print in main)\n",
        "       wmma::store_matrix_sync(c + cRow + cCol * ldc, c_frag, ldc, wmma::mem_col_major);\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "V7i8xKaXVTTO"
      },
      "id": "V7i8xKaXVTTO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda_group_save -g \"source\" -n \"main.cu\"\n",
        "\n",
        "/* THE FOLLOWING CODE TAKEN FROM THE FOLLOWING RESOURCE BY NVIDIA:\n",
        " * https://github.com/NVIDIA-developer-blog/code-samples/blob/master/posts/tensor-cores/simpleTensorCoreGEMM.cu\n",
        "*/\n",
        "\n",
        "/* Copyright (c) 1993-2017, NVIDIA CORPORATION. All rights reserved.\n",
        " *\n",
        " * Redistribution and use in source and binary forms, with or without\n",
        " * modification, are permitted provided that the following conditions\n",
        " * are met:\n",
        " *  * Redistributions of source code must retain the above copyright\n",
        " *    notice, this list of conditions and the following disclaimer.\n",
        " *  * Redistributions in binary form must reproduce the above copyright\n",
        " *    notice, this list of conditions and the following disclaimer in the\n",
        " *    documentation and/or other materials provided with the distribution.\n",
        " *  * Neither the name of NVIDIA CORPORATION nor the names of its\n",
        " *    contributors may be used to endorse or promote products derived\n",
        " *    from this software without specific prior written permission.\n",
        " *\n",
        " * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS ``AS IS'' AND ANY\n",
        " * EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
        " * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n",
        " * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR\n",
        " * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n",
        " * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n",
        " * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n",
        " * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY\n",
        " * OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n",
        " * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
        " * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
        " */\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <curand.h>\n",
        "#include <cublas_v2.h>\n",
        "\n",
        "#include \"error_checking.h\"\n",
        "#include \"wmma_fp16_kernel.h\"\n",
        "#include \"wmma_fp32_kernel.h\"\n",
        "\n",
        "\n",
        "#include <mma.h>\n",
        "using namespace nvcuda;\n",
        "\n",
        "// Must be multiples of 16 for wmma code to work\n",
        "#define MATRIX_M 16\n",
        "#define MATRIX_N 16\n",
        "#define MATRIX_K 16\n",
        "\n",
        "\n",
        "// The only dimensions currently supported by WMMA\n",
        "const int WMMA_M = 16;\n",
        "const int WMMA_N = 16;\n",
        "const int WMMA_K = 16;\n",
        "\n",
        "\n",
        "__global__ void convertFp32ToFp16 (half *out, float *in, int n) {\n",
        "   int idx = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "   if (idx < n) {\n",
        "      out[idx] = in[idx];\n",
        "   }\n",
        "}\n",
        "\n",
        "int main(int argc, char* argv[]) {\n",
        "   float *a_fp32;\n",
        "   float *b_fp32;\n",
        "   half *a_fp16;\n",
        "   half *b_fp16;\n",
        "\n",
        "   float *c;\n",
        "   float *c_wmma;\n",
        "   float *c_host_wmma;\n",
        "\n",
        "   half *c16;\n",
        "   half *c16_wmma;\n",
        "   half *c16_host_wmma;\n",
        "\n",
        "   curandGenerator_t gen;\n",
        "\n",
        "   cudaEvent_t startWMMA;\n",
        "   cudaEvent_t stopWMMA;\n",
        "\n",
        "   cudaErrCheck(cudaEventCreate(&startWMMA));\n",
        "   cudaErrCheck(cudaEventCreate(&stopWMMA));\n",
        "\n",
        "   // Use tensor cores\n",
        "   cudaErrCheck(cudaMalloc((void**)&a_fp32, MATRIX_M * MATRIX_K * sizeof(float)));\n",
        "   cudaErrCheck(cudaMalloc((void**)&b_fp32, MATRIX_K * MATRIX_N * sizeof(float)));\n",
        "   cudaErrCheck(cudaMalloc((void**)&a_fp16, MATRIX_M * MATRIX_K * sizeof(half)));\n",
        "   cudaErrCheck(cudaMalloc((void**)&b_fp16, MATRIX_K * MATRIX_N * sizeof(half)));\n",
        "\n",
        "   cudaErrCheck(cudaMalloc((void**)&c, MATRIX_M * MATRIX_N * sizeof(float)));\n",
        "   cudaErrCheck(cudaMalloc((void**)&c_wmma, MATRIX_M * MATRIX_N * sizeof(float)));\n",
        "   cudaErrCheck(cudaMalloc((void**)&c16, MATRIX_M * MATRIX_N * sizeof(half)));\n",
        "   cudaErrCheck(cudaMalloc((void**)&c16_wmma, MATRIX_M * MATRIX_N * sizeof(half)));\n",
        "\n",
        "   c_host_wmma = (float*)malloc(MATRIX_M * MATRIX_N * sizeof(float));\n",
        "   c16_host_wmma = (half*)malloc(MATRIX_M * MATRIX_N * sizeof(half));\n",
        "\n",
        "   curandErrCheck(curandCreateGenerator(&gen, CURAND_RNG_PSEUDO_DEFAULT));\n",
        "   curandErrCheck(curandSetPseudoRandomGeneratorSeed(gen, 1337ULL));\n",
        "\n",
        "   curandErrCheck(curandGenerateUniform(gen, a_fp32, MATRIX_M * MATRIX_K));\n",
        "   curandErrCheck(curandGenerateUniform(gen, b_fp32, MATRIX_K * MATRIX_N));\n",
        "\n",
        "   // curand doesn't currently support fp16 so we generate in fp32 and convert to fp16.\n",
        "   convertFp32ToFp16 <<< (MATRIX_M * MATRIX_K + 255) / 256, 256 >>> (a_fp16, a_fp32, MATRIX_M * MATRIX_K);\n",
        "   convertFp32ToFp16 <<< (MATRIX_K * MATRIX_N + 255) / 256, 256 >>> (b_fp16, b_fp32, MATRIX_K * MATRIX_N);\n",
        "\n",
        "   curandErrCheck(curandGenerateUniform(gen, c, MATRIX_M * MATRIX_N));\n",
        "   convertFp32ToFp16 <<< (MATRIX_M * MATRIX_K + 255) / 256, 256 >>> (c16, c, MATRIX_M * MATRIX_K);\n",
        "\n",
        "   curandErrCheck(curandDestroyGenerator(gen));\n",
        "\n",
        "   cudaErrCheck(cudaMemcpy(c_wmma, c, MATRIX_M * MATRIX_N * sizeof(float), cudaMemcpyDeviceToDevice));\n",
        "   cudaErrCheck(cudaMemcpy(c16_wmma, c16, MATRIX_M * MATRIX_N * sizeof(half), cudaMemcpyDeviceToDevice));\n",
        "\n",
        "   printf(\"\\nM = %d, N = %d, K = %d.\\n\\n\", MATRIX_M, MATRIX_N, MATRIX_K);\n",
        "\n",
        "   // Dimensions for kernel\n",
        "   dim3 gridDim;\n",
        "   dim3 blockDim;\n",
        "   // blockDim.x must be a multple of warpSize\n",
        "   // 128x4 means we have 16 warps and a block computes a 64x64 output tile\n",
        "   blockDim.x = 128;\n",
        "   blockDim.y = 4;\n",
        "\n",
        "   gridDim.x = (MATRIX_M + (WMMA_M * blockDim.x / 32 - 1)) / (WMMA_M * blockDim.x / 32);\n",
        "   gridDim.y = (MATRIX_N + WMMA_N * blockDim.y - 1) / (WMMA_N * blockDim.y);\n",
        "\n",
        "   printf(\"Launching our GEMM WMMA Kernel...\\n\");\n",
        "   gemm_wmma_half<<<gridDim, blockDim>>>(c16_wmma, MATRIX_M, MATRIX_N, MATRIX_K, a_fp16, b_fp16);\n",
        "   gemm_wmma_full<<<gridDim, blockDim>>>(c_wmma, MATRIX_M, MATRIX_N, MATRIX_K, a_fp32, b_fp32);\n",
        "   cudaErrCheck(cudaEventRecord(stopWMMA));\n",
        "   cudaErrCheck(cudaEventSynchronize(stopWMMA));\n",
        "   cudaDeviceSynchronize();\n",
        "\n",
        "   cudaErrCheck(cudaEventDestroy(startWMMA));\n",
        "   cudaErrCheck(cudaEventDestroy(stopWMMA));\n",
        "\n",
        "   cudaErrCheck(cudaMemcpy(c_host_wmma, c_wmma, MATRIX_M * MATRIX_N * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "   cudaErrCheck(cudaMemcpy(c16_host_wmma, c16_wmma, MATRIX_M * MATRIX_N * sizeof(half), cudaMemcpyDeviceToHost));\n",
        "\n",
        "   // Print resulting c Matrix\n",
        "   printf(\"Contents of c after GEMM has run:\\n\");\n",
        "   for(int a = 0; a < MATRIX_M; a++) {\n",
        "      for(int b = 0; b < MATRIX_N; b++)\n",
        "        {\n",
        "          printf(\"%f \", c_host_wmma[MATRIX_M*a + b]);\n",
        "        }\n",
        "      printf(\"\\n\");\n",
        "   }\n",
        "\n",
        "   printf(\"\\n\");\n",
        "\n",
        "   // Print resulting c16 Matrix\n",
        "   printf(\"Contents of c16 after GEMM has run:\\n\");\n",
        "   for(int a = 0; a < MATRIX_M; a++) {\n",
        "      for(int b = 0; b < MATRIX_N; b++)\n",
        "        {\n",
        "          printf(\"%f \", c16_host_wmma[MATRIX_M*a + b]);\n",
        "        }\n",
        "      printf(\"\\n\");\n",
        "   }\n",
        "\n",
        "   cudaErrCheck(cudaFree(a_fp32));\n",
        "   cudaErrCheck(cudaFree(b_fp32));\n",
        "   cudaErrCheck(cudaFree(a_fp16));\n",
        "   cudaErrCheck(cudaFree(b_fp16));\n",
        "\n",
        "   cudaErrCheck(cudaFree(c));\n",
        "   cudaErrCheck(cudaFree(c_wmma));\n",
        "   free(c_host_wmma);\n",
        "   cudaErrCheck(cudaFree(c16));\n",
        "   cudaErrCheck(cudaFree(c16_wmma));\n",
        "   free(c16_host_wmma);\n",
        "\n",
        "   cudaErrCheck(cudaDeviceReset());\n",
        "   return 0;\n",
        "}"
      ],
      "metadata": {
        "id": "oZwhmTMYbaUy"
      },
      "id": "oZwhmTMYbaUy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cuda_group_run --group \"source\" --compiler-args \"-O3 -g -std=c++20 -arch=sm_75 -lcublas -lcurand\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MP4f4tsciyO",
        "outputId": "21f16a73-7266-4235-fd5f-47700ceda858"
      },
      "id": "8MP4f4tsciyO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "M = 16, N = 16, K = 16.\n",
            "\n",
            "Launching our GEMM WMMA Kernel...\n",
            "Contents of c after GEMM has run:\n",
            "16.762325 16.204956 16.689005 16.297457 16.105873 16.241302 16.541136 16.285084 16.636999 16.936016 16.282730 16.186352 16.935562 16.656845 16.622757 16.024006 \n",
            "16.779079 16.504194 16.839464 16.393778 16.609661 16.244621 16.711142 16.570696 16.747826 16.447248 16.247244 16.272766 16.762253 16.301880 16.475513 16.539602 \n",
            "16.954824 16.615040 16.003706 16.578810 16.778944 16.111719 16.382700 16.980839 16.081549 16.658381 16.171062 16.156496 16.482357 16.980335 16.585537 16.762894 \n",
            "16.999601 16.156195 16.413279 16.850931 16.182135 16.824768 16.365646 16.084660 16.547794 16.378235 16.673759 16.499565 16.223032 16.703112 16.617823 16.919071 \n",
            "16.186605 16.094723 16.163464 16.026201 16.966654 16.206564 16.132320 16.068584 16.209089 16.438320 16.531662 16.715868 16.763878 16.361742 16.684458 16.727577 \n",
            "16.733587 16.886805 16.870750 16.707085 16.059801 16.784218 16.751276 16.835516 16.907932 16.689993 16.321836 16.313957 16.078962 16.046907 16.285547 16.914047 \n",
            "16.983244 16.960581 16.104134 16.940571 16.154604 16.045517 16.528408 16.053291 16.635998 16.109659 16.089470 16.273783 16.251213 16.629770 16.786789 16.075697 \n",
            "16.620213 16.986835 16.601549 16.012234 16.606960 16.626795 16.603077 16.910618 16.332571 16.227865 16.859142 16.300917 16.147293 16.879066 16.488876 16.884256 \n",
            "16.262690 16.435223 16.809498 16.336439 16.557156 16.607016 16.489609 16.984028 16.585833 16.615341 16.897532 16.124075 16.954765 16.143620 16.717640 16.896868 \n",
            "16.045109 16.739803 16.221495 16.225908 16.761148 16.540627 16.158821 16.369951 16.860109 16.194502 16.991478 16.133627 16.589699 16.322315 16.016605 16.838610 \n",
            "16.215361 16.666899 16.127472 16.135666 16.819939 16.187057 16.797035 16.323040 16.582855 16.186104 16.793484 16.487461 16.790838 16.311157 16.771355 16.273851 \n",
            "16.162634 16.521994 16.184456 16.411617 16.958345 16.441154 16.012535 16.457300 16.073952 16.533451 16.128946 16.939049 16.665648 16.416729 16.315134 16.859116 \n",
            "16.631121 16.640890 16.254122 16.358473 16.230978 16.068415 16.433466 16.701395 16.324150 16.625893 16.201157 16.114996 16.079210 16.489462 16.542046 16.675777 \n",
            "16.122938 16.322433 16.940254 16.490871 16.752136 16.291473 16.092211 16.221838 16.207785 16.747997 16.230625 16.202431 16.704445 16.451944 16.277382 16.197483 \n",
            "16.363731 16.151716 16.334131 16.221199 16.522486 16.689631 16.118210 16.089397 16.858595 16.401676 16.625134 16.635412 16.794821 16.271086 16.857536 16.276169 \n",
            "16.548489 16.806269 16.252083 16.778381 16.468214 16.442137 16.727598 16.479364 16.511066 16.698166 16.193222 16.723942 16.168407 16.422123 16.804033 16.960691 \n",
            "\n",
            "Contents of c16 after GEMM has run:\n",
            "16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 \n",
            "16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 \n",
            "16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 \n",
            "16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 \n",
            "16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 \n",
            "16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 \n",
            "16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 \n",
            "16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 \n",
            "16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 \n",
            "16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 \n",
            "16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 \n",
            "16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 \n",
            "16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 \n",
            "16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 \n",
            "16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 \n",
            "16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cuda_group_run --group \"source\" --compiler-args \"-O3 -g -std=c++20 -arch=sm_75 -lcublas -lcurand\" --profiler ncu --profile --profiler-args \"--section SpeedOfLight\""
      ],
      "metadata": {
        "id": "VRKtM2_7twtq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c1ed6b3-6518-4b40-fa06-e14377f24659"
      },
      "id": "VRKtM2_7twtq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==PROF== Connected to process 1060 (/tmp/tmp9h3k53d5/source/cuda_exec.out)\n",
            "==PROF== Profiling \"generate_seed_pseudo\" - 0: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"gen_sequenced\" - 1: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"gen_sequenced\" - 2: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"convertFp32ToFp16\" - 3: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"convertFp32ToFp16\" - 4: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"gen_sequenced\" - 5: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"convertFp32ToFp16\" - 6: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"gemm_wmma_half\" - 7: 0%....50%....100% - 8 passes\n",
            "==PROF== Profiling \"gemm_wmma_full\" - 8: 0%....50%....100% - 8 passes\n",
            "\n",
            "M = 16, N = 16, K = 16.\n",
            "\n",
            "Launching our GEMM WMMA Kernel...\n",
            "Contents of c after GEMM has run:\n",
            "16.762325 16.204956 16.689005 16.297457 16.105873 16.241302 16.541136 16.285084 16.636999 16.936016 16.282730 16.186352 16.935562 16.656845 16.622757 16.024006 \n",
            "16.779079 16.504194 16.839464 16.393778 16.609661 16.244621 16.711142 16.570696 16.747826 16.447248 16.247244 16.272766 16.762253 16.301880 16.475513 16.539602 \n",
            "16.954824 16.615040 16.003706 16.578810 16.778944 16.111719 16.382700 16.980839 16.081549 16.658381 16.171062 16.156496 16.482357 16.980335 16.585537 16.762894 \n",
            "16.999601 16.156195 16.413279 16.850931 16.182135 16.824768 16.365646 16.084660 16.547794 16.378235 16.673759 16.499565 16.223032 16.703112 16.617823 16.919071 \n",
            "16.186605 16.094723 16.163464 16.026201 16.966654 16.206564 16.132320 16.068584 16.209089 16.438320 16.531662 16.715868 16.763878 16.361742 16.684458 16.727577 \n",
            "16.733587 16.886805 16.870750 16.707085 16.059801 16.784218 16.751276 16.835516 16.907932 16.689993 16.321836 16.313957 16.078962 16.046907 16.285547 16.914047 \n",
            "16.983244 16.960581 16.104134 16.940571 16.154604 16.045517 16.528408 16.053291 16.635998 16.109659 16.089470 16.273783 16.251213 16.629770 16.786789 16.075697 \n",
            "16.620213 16.986835 16.601549 16.012234 16.606960 16.626795 16.603077 16.910618 16.332571 16.227865 16.859142 16.300917 16.147293 16.879066 16.488876 16.884256 \n",
            "16.262690 16.435223 16.809498 16.336439 16.557156 16.607016 16.489609 16.984028 16.585833 16.615341 16.897532 16.124075 16.954765 16.143620 16.717640 16.896868 \n",
            "16.045109 16.739803 16.221495 16.225908 16.761148 16.540627 16.158821 16.369951 16.860109 16.194502 16.991478 16.133627 16.589699 16.322315 16.016605 16.838610 \n",
            "16.215361 16.666899 16.127472 16.135666 16.819939 16.187057 16.797035 16.323040 16.582855 16.186104 16.793484 16.487461 16.790838 16.311157 16.771355 16.273851 \n",
            "16.162634 16.521994 16.184456 16.411617 16.958345 16.441154 16.012535 16.457300 16.073952 16.533451 16.128946 16.939049 16.665648 16.416729 16.315134 16.859116 \n",
            "16.631121 16.640890 16.254122 16.358473 16.230978 16.068415 16.433466 16.701395 16.324150 16.625893 16.201157 16.114996 16.079210 16.489462 16.542046 16.675777 \n",
            "16.122938 16.322433 16.940254 16.490871 16.752136 16.291473 16.092211 16.221838 16.207785 16.747997 16.230625 16.202431 16.704445 16.451944 16.277382 16.197483 \n",
            "16.363731 16.151716 16.334131 16.221199 16.522486 16.689631 16.118210 16.089397 16.858595 16.401676 16.625134 16.635412 16.794821 16.271086 16.857536 16.276169 \n",
            "16.548489 16.806269 16.252083 16.778381 16.468214 16.442137 16.727598 16.479364 16.511066 16.698166 16.193222 16.723942 16.168407 16.422123 16.804033 16.960691 \n",
            "\n",
            "Contents of c16 after GEMM has run:\n",
            "16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 \n",
            "16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 \n",
            "16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 \n",
            "16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 \n",
            "16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 \n",
            "16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 \n",
            "16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 \n",
            "16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 \n",
            "16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 1==PROF== Disconnected from process 1060\n",
            "6.960691 16.960691 16.960691 16.960691 16.960691 16.960691 \n",
            "16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 \n",
            "16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 \n",
            "16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 \n",
            "16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 \n",
            "16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 \n",
            "16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 \n",
            "16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 16.960691 \n",
            "[1060] cuda_exec.out@127.0.0.1\n",
            "  void generate_seed_pseudo<rng_config<curandStateXORWOW, (curandOrdering)101>>(unsigned long long, unsigned long long, unsigned long long, curandOrdering, curandStateXORWOW *, unsigned int *) (64, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         5.29\n",
            "    SM Frequency            cycle/usecond       619.56\n",
            "    Elapsed Cycles                  cycle      158,865\n",
            "    Memory Throughput                   %        40.82\n",
            "    DRAM Throughput                     %         0.35\n",
            "    Duration                      usecond       256.42\n",
            "    L1/TEX Cache Throughput             %        55.15\n",
            "    L2 Cache Throughput                 %         1.87\n",
            "    SM Active Cycles                cycle   117,582.43\n",
            "    Compute (SM) Throughput             %        40.82\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "  void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>, rng_config<curandStateXORWOW, (curandOrdering)101>>(T1 *, T2 *, unsigned long, unsigned long, T3) (64, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.67\n",
            "    SM Frequency            cycle/usecond       541.42\n",
            "    Elapsed Cycles                  cycle        2,374\n",
            "    Memory Throughput                   %         1.34\n",
            "    DRAM Throughput                     %         1.17\n",
            "    Duration                      usecond         4.38\n",
            "    L1/TEX Cache Throughput             %         3.01\n",
            "    L2 Cache Throughput                 %         1.34\n",
            "    SM Active Cycles                cycle       613.50\n",
            "    Compute (SM) Throughput             %         0.68\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "  void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>, rng_config<curandStateXORWOW, (curandOrdering)101>>(T1 *, T2 *, unsigned long, unsigned long, T3) (64, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.58\n",
            "    SM Frequency            cycle/usecond       541.79\n",
            "    Elapsed Cycles                  cycle        2,376\n",
            "    Memory Throughput                   %         1.31\n",
            "    DRAM Throughput                     %         1.17\n",
            "    Duration                      usecond         4.38\n",
            "    L1/TEX Cache Throughput             %         2.90\n",
            "    L2 Cache Throughput                 %         1.31\n",
            "    SM Active Cycles                cycle       636.80\n",
            "    Compute (SM) Throughput             %         0.68\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "  convertFp32ToFp16(__half *, float *, int) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.83\n",
            "    SM Frequency            cycle/usecond       564.28\n",
            "    Elapsed Cycles                  cycle        2,095\n",
            "    Memory Throughput                   %         0.54\n",
            "    DRAM Throughput                     %         0.15\n",
            "    Duration                      usecond         3.71\n",
            "    L1/TEX Cache Throughput             %        19.29\n",
            "    L2 Cache Throughput                 %         0.54\n",
            "    SM Active Cycles                cycle        23.32\n",
            "    Compute (SM) Throughput             %         0.06\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "  convertFp32ToFp16(__half *, float *, int) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.89\n",
            "    SM Frequency            cycle/usecond       565.06\n",
            "    Elapsed Cycles                  cycle        2,116\n",
            "    Memory Throughput                   %         0.54\n",
            "    DRAM Throughput                     %         0.15\n",
            "    Duration                      usecond         3.74\n",
            "    L1/TEX Cache Throughput             %        19.25\n",
            "    L2 Cache Throughput                 %         0.54\n",
            "    SM Active Cycles                cycle        23.38\n",
            "    Compute (SM) Throughput             %         0.06\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "  void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>, rng_config<curandStateXORWOW, (curandOrdering)101>>(T1 *, T2 *, unsigned long, unsigned long, T3) (64, 1, 1)x(64, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.61\n",
            "    SM Frequency            cycle/usecond       541.83\n",
            "    Elapsed Cycles                  cycle        2,376\n",
            "    Memory Throughput                   %         1.34\n",
            "    DRAM Throughput                     %         1.14\n",
            "    Duration                      usecond         4.38\n",
            "    L1/TEX Cache Throughput             %         3.02\n",
            "    L2 Cache Throughput                 %         1.34\n",
            "    SM Active Cycles                cycle          612\n",
            "    Compute (SM) Throughput             %         0.68\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "  convertFp32ToFp16(__half *, float *, int) (1, 1, 1)x(256, 1, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.69\n",
            "    SM Frequency            cycle/usecond       556.09\n",
            "    Elapsed Cycles                  cycle        2,065\n",
            "    Memory Throughput                   %         0.55\n",
            "    DRAM Throughput                     %         0.16\n",
            "    Duration                      usecond         3.71\n",
            "    L1/TEX Cache Throughput             %        19.44\n",
            "    L2 Cache Throughput                 %         0.55\n",
            "    SM Active Cycles                cycle        23.15\n",
            "    Compute (SM) Throughput             %         0.06\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "  gemm_wmma_half(__half *, int, int, int) (1, 1, 1)x(128, 4, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.69\n",
            "    SM Frequency            cycle/usecond       547.83\n",
            "    Elapsed Cycles                  cycle        2,525\n",
            "    Memory Throughput                   %         0.67\n",
            "    DRAM Throughput                     %         0.08\n",
            "    Duration                      usecond         4.61\n",
            "    L1/TEX Cache Throughput             %        12.75\n",
            "    L2 Cache Throughput                 %         0.67\n",
            "    SM Active Cycles                cycle        34.90\n",
            "    Compute (SM) Throughput             %         0.24\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "  gemm_wmma_full(float *, int, int, int) (1, 1, 1)x(128, 4, 1), Context 1, Stream 7, Device 0, CC 7.5\n",
            "    Section: GPU Speed Of Light Throughput\n",
            "    ----------------------- ------------- ------------\n",
            "    Metric Name               Metric Unit Metric Value\n",
            "    ----------------------- ------------- ------------\n",
            "    DRAM Frequency          cycle/nsecond         4.69\n",
            "    SM Frequency            cycle/usecond       550.29\n",
            "    Elapsed Cycles                  cycle        2,642\n",
            "    Memory Throughput                   %         0.73\n",
            "    DRAM Throughput                     %         0.12\n",
            "    Duration                      usecond         4.80\n",
            "    L1/TEX Cache Throughput             %        13.00\n",
            "    L2 Cache Throughput                 %         0.73\n",
            "    SM Active Cycles                cycle        37.70\n",
            "    Compute (SM) Throughput             %         0.24\n",
            "    ----------------------- ------------- ------------\n",
            "\n",
            "    OPT   This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full      \n",
            "          waves across all SMs. Look at Launch Statistics for more details.                                             \n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}