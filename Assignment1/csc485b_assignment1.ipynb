{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dclGnLGAgbtH",
        "outputId": "3b7eaebc-9c77-4bc2-c315-184d06ffff3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-jwmwnxph\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-jwmwnxph\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit 28f872a2f99a1b201bcd0db14fdbc5a496b9bfd7\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: nvcc4jupyter\n",
            "  Building wheel for nvcc4jupyter (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvcc4jupyter: filename=nvcc4jupyter-1.2.1-py3-none-any.whl size=10743 sha256=3a570d3ab74e9051a615aa78d54722c78a69a0c83a1d407fccde14c37a30e96c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-0u46w5qn/wheels/a8/b9/18/23f8ef71ceb0f63297dd1903aedd067e6243a68ea756d6feea\n",
            "Successfully built nvcc4jupyter\n",
            "Installing collected packages: nvcc4jupyter\n",
            "Successfully installed nvcc4jupyter-1.2.1\n",
            "Detected platform \"Colab\". Running its setup...\n",
            "Source files will be saved in \"/tmp/tmpkxbksdcx\".\n"
          ]
        }
      ],
      "source": [
        "# Load the extension that allows us to compile CUDA code in python notebooks\n",
        "# Documentation is here: https://nvcc4jupyter.readthedocs.io/en/latest/\n",
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
        "%load_ext nvcc4jupyter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda_group_save -g \"source\" -n \"data_types.h\"\n",
        "/**\n",
        " * A collection of commonly used data types throughout this project.\n",
        " */\n",
        "#pragma once\n",
        "\n",
        "#include <stdint.h> // uint32_t\n",
        "\n",
        "using element_t = uint32_t;"
      ],
      "metadata": {
        "id": "VVbDQthwogQF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda_group_save -g \"source\" -n \"cuda_common.h\"\n",
        "/**\n",
        " * Standard macros that can be useful for error checking.\n",
        " * https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__ERROR.html\n",
        " */\n",
        "#pragma once\n",
        "\n",
        "#include <cuda.h>\n",
        "\n",
        "#define CUDA_CALL(exp)                                       \\\n",
        "    do {                                                     \\\n",
        "        cudaError res = (exp);                               \\\n",
        "        if(res != cudaSuccess) {                             \\\n",
        "            printf(\"Error at %s:%d\\n %s\\n\",                  \\\n",
        "                __FILE__,__LINE__, cudaGetErrorString(res)); \\\n",
        "           exit(EXIT_FAILURE);                               \\\n",
        "        }                                                    \\\n",
        "    } while(0)\n",
        "\n",
        "#define CHECK_ERROR(msg)                                             \\\n",
        "    do {                                                             \\\n",
        "        cudaError_t err = cudaGetLastError();                        \\\n",
        "        if(cudaSuccess != err) {                                     \\\n",
        "            printf(\"Error (%s) at %s:%d\\n %s\\n\",                     \\\n",
        "                (msg), __FILE__, __LINE__, cudaGetErrorString(err)); \\\n",
        "            exit(EXIT_FAILURE);                                      \\\n",
        "        }                                                            \\\n",
        "    } while (0)"
      ],
      "metadata": {
        "id": "ZqET4uI2ggwf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda_group_save -g \"source\" -n \"data_generator.h\"\n",
        "/**\n",
        " * Functions for generating random input data with a fixed seed\n",
        " */\n",
        "#pragma once\n",
        "\n",
        "#include <random>  // for std::mt19937, std::uniform_int_distribution\n",
        "#include <vector>\n",
        "\n",
        "#include \"data_types.h\"\n",
        "\n",
        "namespace csc485b {\n",
        "namespace a1 {\n",
        "\n",
        "/**\n",
        " * Generates and returns a vector of random uniform data of a given length, n,\n",
        " * for any integral type. Input range will be [0, 2n].\n",
        " */\n",
        "template < typename T >\n",
        "std::vector< T > generate_uniform( std::size_t n )\n",
        "{\n",
        "    // for details of random number generation, see:\n",
        "    // https://en.cppreference.com/w/cpp/numeric/random/uniform_int_distribution\n",
        "    std::size_t random_seed = 20240916;  // use magic seed\n",
        "    std::mt19937 rng( random_seed );     // use mersenne twister generator\n",
        "    std::uniform_int_distribution<> distrib(0, 2 * n);\n",
        "\n",
        "    std::vector< T > random_data( n ); // init array\n",
        "    std::generate( std::begin( random_data )\n",
        "                 , std::end  ( random_data )\n",
        "                 , [ &rng, &distrib ](){ return static_cast< T >( distrib( rng ) ); });\n",
        "\n",
        "    return random_data;\n",
        "}\n",
        "\n",
        "} // namespace a1\n",
        "} // namespace csc485b"
      ],
      "metadata": {
        "id": "GY0L7rKhoVaZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda_group_save -g \"source\" -n \"algorithm_choices.h\"\n",
        "#pragma once\n",
        "\n",
        "#include <vector>\n",
        "\n",
        "#include \"data_types.h\"\n",
        "\n",
        "namespace csc485b {\n",
        "namespace a1 {\n",
        "namespace cpu {\n",
        "\n",
        "void run_cpu_baseline( std::vector< element_t > data, std::size_t switch_at, std::size_t n );\n",
        "\n",
        "} // namespace cpu\n",
        "\n",
        "\n",
        "namespace gpu {\n",
        "\n",
        "void run_gpu_soln( std::vector< element_t > data, std::size_t switch_at, std::size_t n );\n",
        "\n",
        "} // namespace gpu\n",
        "} // namespace a1\n",
        "} // namespace csc485b"
      ],
      "metadata": {
        "id": "IJOKRZuCkDh2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda_group_save -g \"source\" -n \"cpu_baseline.cu\"\n",
        "/**\n",
        " * CPU methods that the GPU should outperform.\n",
        " */\n",
        "\n",
        "#include \"algorithm_choices.h\"\n",
        "\n",
        "#include <algorithm> // std::sort()\n",
        "#include <chrono>    // for timing\n",
        "#include <iostream>  // std::cout, std::endl\n",
        "\n",
        "namespace csc485b {\n",
        "namespace a1      {\n",
        "namespace cpu     {\n",
        "\n",
        "/**\n",
        " * Simple solution that just sorts the whole array with a built-in sort\n",
        " * function and then resorts the last portion in the opposing order with\n",
        " * a second call to that same built-in sort function.\n",
        " */\n",
        "void opposing_sort( element_t * data, std::size_t invert_at_pos, std::size_t num_elements )\n",
        "{\n",
        "    std::sort( data, data + num_elements, std::less< element_t >{} );\n",
        "    std::sort( data + invert_at_pos, data + num_elements, std::greater< element_t >{} );\n",
        "}\n",
        "\n",
        "/**\n",
        " * Run the single-threaded CPU baseline that students are supposed to outperform\n",
        " * in order to obtain higher grades on this assignment. Times the execution and\n",
        " * prints to the standard output (e.g., the screen) that \"wall time.\" Note that\n",
        " * the functions takes the input by value so as to not perturb the original data\n",
        " * in place.\n",
        " */\n",
        "void run_cpu_baseline( std::vector< element_t > data, std::size_t switch_at, std::size_t n )\n",
        "{\n",
        "    auto const cpu_start = std::chrono::high_resolution_clock::now();\n",
        "    opposing_sort( data.data(), switch_at, n );\n",
        "    auto const cpu_end = std::chrono::high_resolution_clock::now();\n",
        "\n",
        "    std::cout << \"CPU Baseline time: \"\n",
        "              << std::chrono::duration_cast<std::chrono::nanoseconds>(cpu_end - cpu_start).count()\n",
        "              << \" ns\" << std::endl;\n",
        "\n",
        "    for( auto const x : data ) std::cout << x << \" \"; std::cout << std::endl;\n",
        "}\n",
        "\n",
        "} // namespace cpu\n",
        "} // namespace a1\n",
        "} // namespace csc485b"
      ],
      "metadata": {
        "id": "V3lAuiBEhKjc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda_group_save -g \"source\" -n \"gpu_solution.cu\"\n",
        "/**\n",
        " * The file in which you will implement your GPU solutions!\n",
        " */\n",
        "\n",
        "#include \"algorithm_choices.h\"\n",
        "\n",
        "#include <chrono>    // for timing\n",
        "#include <iostream>  // std::cout, std::endl\n",
        "\n",
        "#include \"cuda_common.h\"\n",
        "\n",
        "namespace csc485b {\n",
        "namespace a1      {\n",
        "namespace gpu     {\n",
        "\n",
        "/**\n",
        " * The CPU baseline benefits from warm caches because the data was generated on\n",
        " * the CPU. Run the data through the GPU once with some arbitrary logic to\n",
        " * ensure that the GPU cache is warm too and the comparison is more fair.\n",
        " */\n",
        "__global__\n",
        "void warm_the_gpu( element_t * data, std::size_t invert_at_pos, std::size_t num_elements )\n",
        "{\n",
        "    int const th_id = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    // We know this will never be true, because of the data generator logic,\n",
        "    // but I doubt that the compiler will figure it out. Thus every element\n",
        "    // should be read, but none of them should be modified.\n",
        "    if( th_id < num_elements && data[ th_id ] > num_elements * 100 )\n",
        "    {\n",
        "        ++data[ th_id ]; // should not be possible.\n",
        "    }\n",
        "}\n",
        "\n",
        "/**\n",
        " * Your solution. Should match the CPU output.\n",
        " */\n",
        "__global__\n",
        "void opposing_sort( element_t * data, std::size_t invert_at_pos, std::size_t num_elements )\n",
        "{\n",
        "    int const th_id = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\n",
        "    if( th_id < num_elements )\n",
        "    {\n",
        "        // IMPLEMENT ME!\n",
        "        return;\n",
        "    }\n",
        "}\n",
        "\n",
        "/**\n",
        " * Performs all the logic of allocating device vectors and copying host/input\n",
        " * vectors to the device. Times the opposing_sort() kernel with wall time,\n",
        " * but excludes set up and tear down costs such as mallocs, frees, and memcpies.\n",
        " */\n",
        "void run_gpu_soln( std::vector< element_t > data, std::size_t switch_at, std::size_t n )\n",
        "{\n",
        "    // Kernel launch configurations. Feel free to change these.\n",
        "    // This is set to maximise the size of a thread block on a T4, but it hasn't\n",
        "    // been tuned. It's not known if this is optimal.\n",
        "    std::size_t const threads_per_block = 1024;\n",
        "    std::size_t const num_blocks =  ( n + threads_per_block - 1 ) / threads_per_block;\n",
        "\n",
        "    // Allocate arrays on the device/GPU\n",
        "    element_t * d_data;\n",
        "    cudaMalloc( (void**) & d_data, sizeof( element_t ) * n );\n",
        "    CHECK_ERROR(\"Allocating input array on device\");\n",
        "\n",
        "    // Copy the input from the host to the device/GPU\n",
        "    cudaMemcpy( d_data, data.data(), sizeof( element_t ) * n, cudaMemcpyHostToDevice );\n",
        "    CHECK_ERROR(\"Copying input array to device\");\n",
        "\n",
        "    // Warm the cache on the GPU for a more fair comparison\n",
        "    warm_the_gpu<<< num_blocks, threads_per_block>>>( d_data, switch_at, n );\n",
        "\n",
        "    // Time the execution of the kernel that you implemented\n",
        "    auto const kernel_start = std::chrono::high_resolution_clock::now();\n",
        "    opposing_sort<<< num_blocks, threads_per_block>>>( d_data, switch_at, n );\n",
        "    auto const kernel_end = std::chrono::high_resolution_clock::now();\n",
        "    CHECK_ERROR(\"Executing kernel on device\");\n",
        "\n",
        "    // After the timer ends, copy the result back, free the device vector,\n",
        "    // and echo out the timings and the results.\n",
        "    cudaMemcpy( data.data(), d_data, sizeof( element_t ) * n, cudaMemcpyDeviceToHost );\n",
        "    CHECK_ERROR(\"Transferring result back to host\");\n",
        "    cudaFree( d_data );\n",
        "    CHECK_ERROR(\"Freeing device memory\");\n",
        "\n",
        "    std::cout << \"GPU Solution time: \"\n",
        "              << std::chrono::duration_cast<std::chrono::nanoseconds>(kernel_end - kernel_start).count()\n",
        "              << \" ns\" << std::endl;\n",
        "\n",
        "    for( auto const x : data ) std::cout << x << \" \"; std::cout << std::endl;\n",
        "}\n",
        "\n",
        "} // namespace gpu\n",
        "} // namespace a1\n",
        "} // namespace csc485b"
      ],
      "metadata": {
        "id": "bjTbQ3EO2NwQ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda_group_save -g \"source\" -n \"main.cu\"\n",
        "/**\n",
        " * Driver for the benchmark comparison. Generates random data,\n",
        " * runs the CPU baseline, and then runs your code.\n",
        " */\n",
        "\n",
        "#include <cstddef>  // std::size_t type\n",
        "#include <iostream> // std::cout, std::endl\n",
        "#include <vector>\n",
        "\n",
        "#include \"algorithm_choices.h\"\n",
        "#include \"data_generator.h\"\n",
        "#include \"data_types.h\"\n",
        "#include \"cuda_common.h\"\n",
        "\n",
        "int main()\n",
        "{\n",
        "    std::size_t const n = 4;\n",
        "    std::size_t const switch_at = 3 * ( n >> 2 ) ;\n",
        "\n",
        "    auto data = csc485b::a1::generate_uniform< element_t >( n );\n",
        "    csc485b::a1::cpu::run_cpu_baseline( data, switch_at, n );\n",
        "    csc485b::a1::gpu::run_gpu_soln( data, switch_at, n );\n",
        "\n",
        "    return EXIT_SUCCESS;\n",
        "}"
      ],
      "metadata": {
        "id": "IRvVeK-QifnZ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cuda_group_run --group \"source\" --compiler-args \"-O0 -g -std=c++20 -arch=sm_75\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7F0eVsGjUNp",
        "outputId": "ee81257b-337a-4d7c-df77-37e97a569b78"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU Baseline time: 923 ns\n",
            "0 1 3 5 \n",
            "GPU Solution time: 13754 ns\n",
            "0 3 1 5 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K0Yqomwu6WsP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}